{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import missingno as msno\n",
    "from pprint import pprint\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import scipy.stats as st\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# 固定随机种子，稳定模型效果\n",
    "random.seed(2021) \n",
    "np.random.seed(2021)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rc(\"font\",family=\"SimHei\",size=\"10\")\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "# 数据集路径\n",
    "root = Path(os.getcwd()).resolve().parent / \"data\"\n",
    "root2 = Path(os.getcwd()).resolve().parent / \"pic\"\n",
    "train = root / \"Q3_train.csv\"\n",
    "data_df = pd.read_csv(train)\n",
    "\n",
    "# index2name存放的是所有特征的名称\n",
    "index2name = [\"carid\", \"pushDate\", \"pushPrice\", \"updatePriceTimeJson\", \"pullDate\", \"withdrawDate\", \"展销时间\",\n",
    "              \"品牌 id\", \"车系 id\", \"车型 id\", \"里程\", \"车辆颜色\", \"车辆所在城市 id\", \"国标码\",\n",
    "              \"过户次数\", \"载客人数\", \"注册日期\", \"上牌日期\", \"国别\", \"厂商类型\", \"年款\", \"排量\", \"变速箱\", \"燃油类型\",\n",
    "              \"新车价\"]\n",
    "\n",
    "# category存放的是类别特征的名称\n",
    "category = [\"品牌 id\", \"车系 id\", \"车型 id\", \"车辆颜色\", \"车辆所在城市 id\", \"厂商类型\", \"燃油类型\"] # 类别类型特征\n",
    "\n",
    "# numerical存放的是数值特征的名称\n",
    "numerical = [\"Feature_5\", \"年款\", \"排量\", \"变速箱\", \"Feature_2\", \"Feature_13\", \"载客人数\", \"新车价\", \"target\", \"fday\", \"pushPrice\"] #  \"里程\", \"厂商类型\"实数类型特征, \"Feature_12\", \"Feature_8\"\n",
    "\n",
    "# cross_num存放的是要进行特征交叉的特征的名称\n",
    "cross_num = [\"新车价\", \"Feature_2\", \"Feature_12_1\"]\n",
    "\n",
    "# 匿名特征为Feature_i，此处将15个匿名特征加入到index2name中\n",
    "for _ in range(1, 16):\n",
    "    index2name.append(\"Feature_\"+str(_))\n",
    "index2name.append(\"target\")\n",
    "index2name.append(\"fday\")\n",
    "index2name.append(\"sday\")\n",
    "index2name.append(\"成交周期\")\n",
    "\n",
    "\n",
    "sFea = {}\n",
    "cx = data_df.groupby(\"车系 id\")\n",
    "for a, b in cx:\n",
    "    sFea[a] = list(set(b[\"车型 id\"]))\n",
    "\n",
    "def metric(y_, y):\n",
    "    ape = np.abs(y_ - y)\n",
    "    mape = np.mean(ape)\n",
    "    return mape\n",
    "\n",
    "data_df = data_df[data_df[\"pullDate\"] == data_df[\"withdrawDate\"]]\n",
    "\n",
    "data_df[\"updatePriceTimeJson\"] = data_df[[\"pushDate\", \"pushPrice\", \"updatePriceTimeJson\"]].apply(lambda x: {str(x[\"pushDate\"]): x[\"pushPrice\"]} if len(x[\"updatePriceTimeJson\"]) == 2 else dict({str(x[\"pushDate\"]): x[\"pushPrice\"]}, **json.loads(x[\"updatePriceTimeJson\"])), axis=1)\n",
    "\n",
    "data_df = data_df[data_df[\"target\"] <= 10000]\n",
    "data_df.sort_values(by=\"展销时间\", inplace=True)\n",
    "\n",
    "\n",
    "data_df[\"展销时间\"] = pd.to_datetime(data_df[\"展销时间\"])\n",
    "data_df[\"注册日期\"] = pd.to_datetime(data_df[\"注册日期\"])\n",
    "data_df[\"上牌日期\"] = pd.to_datetime(data_df[\"上牌日期\"])\n",
    "data_df[\"pushDate\"] = pd.to_datetime(data_df[\"pushDate\"])\n",
    "data_df[\"pullDate\"] = pd.to_datetime(data_df[\"pullDate\"])\n",
    "\n",
    "data_df[\"fday\"] = data_df[[\"pushDate\", \"updatePriceTimeJson\"]].apply(lambda x: (pd.to_datetime(list(x[\"updatePriceTimeJson\"].keys())[-1]) - x[\"pushDate\"]).days, axis=1)\n",
    "data_df[\"sday\"] = data_df[\"成交周期\"]  - data_df[\"fday\"]\n",
    "\n",
    "data_df[\"成交周期2\"] = data_df[\"成交周期\"]\n",
    "del data_df[\"成交周期\"]\n",
    "data_df[\"成交周期\"] = data_df[\"成交周期2\"]\n",
    "del data_df[\"成交周期2\"]\n",
    "\n",
    "te = pd.read_csv(root / \"Q3_processed.csv\")\n",
    "te[\"展销时间\"] = pd.to_datetime(te[\"展销时间\"])\n",
    "te[\"注册日期\"] = pd.to_datetime(te[\"注册日期\"])\n",
    "te[\"上牌日期\"] = pd.to_datetime(te[\"上牌日期\"])\n",
    "te[\"pushDate\"] = pd.to_datetime(te[\"pushDate\"])\n",
    "te[\"updatePriceTimeJson\"] = te[[\"pushDate\", \"pushPrice\", \"updatePriceTimeJson\"]].apply(lambda x: {str(x[\"pushDate\"]): x[\"pushPrice\"]} if len(x[\"updatePriceTimeJson\"]) == 2 else dict({str(x[\"pushDate\"]): x[\"pushPrice\"]}, **json.loads(x[\"updatePriceTimeJson\"])), axis=1)\n",
    "\n",
    "# add the null\n",
    "# tr = pd.concat([tr, data_df_null], axis=0)\n",
    "tr = data_df\n",
    "tr = tr[((tr[\"新车价\"]-tr[\"target\"]) / tr[\"新车价\"]) < 0.969]\n",
    "tr = tr[tr[\"新车价\"] > tr[\"target\"]]\n",
    "\n",
    "# 使用众数填补缺省值\n",
    "for _ in tr:\n",
    "    tr[_] = tr[_].fillna(tr[_].value_counts().index[0])\n",
    "\n",
    "import scipy.stats as st\n",
    "y = tr[\"sday\"]\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('Johnson SU')\n",
    "sns.distplot(y, kde=False, fit=st.johnsonsu)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Normal')\n",
    "sns.distplot(y, kde=False, fit=st.norm)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Log Normal')\n",
    "sns.distplot(y, kde=False, fit=st.lognorm)\n",
    "plt.savefig(root2 / \"Q3_0.jpg\")\n",
    "\n",
    "pt_tr = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "tr[\"sday\"] = pt_tr.fit_transform(np.array(tr['sday']).reshape(-1,1))\n",
    "# tr[\"成交周期\"] = np.log(tr[\"成交周期\"])\n",
    "plt.figure(figsize=[20, 10])\n",
    "sns.displot(tr['sday'])\n",
    "plt.savefig(root2 / \"Q3_1.jpg\")\n",
    "\n",
    "# ******************构造新特征********************\n",
    "class FeatureEngine:\n",
    "    def __init__(self):\n",
    "        self.ID = defaultdict(lambda: defaultdict(int))\n",
    "        self.sFea = {}\n",
    "        self.target_cxx = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "    @staticmethod\n",
    "    def add2list(pos, name, stage=\"train\"):\n",
    "        if stage == \"train\":\n",
    "            index2name.insert(pos, name)\n",
    "            numerical.append(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add2list_cat(pos, name, stage=\"train\"):\n",
    "        if stage == \"train\":\n",
    "            index2name.insert(pos, name)\n",
    "            category.append(name)\n",
    "    \n",
    "    # strongFea没啥用，忽略\n",
    "    def strongFea(self, data_df, stage):\n",
    "        if stage == \"train\":\n",
    "            cx = tr.groupby(\"车系 id\")\n",
    "            for a, b in cx:\n",
    "                self.sFea[a] = list(set(b[\"车型 id\"]))\n",
    "    \n",
    "            for cx_idx in self.sFea:\n",
    "                for cxing_idx in self.sFea[cx_idx]:\n",
    "                    if len(data_df[(data_df[\"车系 id\"]==cx_idx) & (data_df[\"车型 id\"]==cxing_idx)]) <= 5:\n",
    "                        self.target_cxx[cx_idx][cxing_idx] = 0\n",
    "                    else:\n",
    "                        self.target_cxx[cx_idx][cxing_idx] = float(data_df[(data_df[\"车系 id\"]==cx_idx) & (data_df[\"车型 id\"]==cxing_idx)][\"target\"].mean())\n",
    "            \n",
    "        data_df.insert(len(data_df.columns)-1, \"车系车型_mean\", data_df.apply(lambda x: self.target_cxx[x[\"车系 id\"]][x[\"车型 id\"]], axis=1))\n",
    "        data_df[\"车系车型_mean\"] = data_df[\"车系车型_mean\"].fillna(data_df[\"车系车型_mean\"].mean())\n",
    "        self.add2list(len(index2name)-1, \"车系车型_mean\", stage)\n",
    "    \n",
    "    # 下面一系列的get函数是类似的，按照某个特征进行聚类，然后求mean，count，max......\n",
    "    def getMean(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_mean\"][list(set(v[fea_name]))[0]] = v[\"target\"].mean()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_mean\", data_df[fea_name].map(self.ID[fea_name+\"_mean\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_mean\", data_df[fea_name].map(self.ID[fea_name+\"_mean\"]))\n",
    "            data_df[fea_name+\"_mean\"] = data_df[fea_name+\"_mean\"].fillna(data_df[fea_name+\"_mean\"].mean())\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_mean\", stage)\n",
    "    \n",
    "    \n",
    "    def getCount(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            self.ID[fea_name+\"_count\"] = dict(data_df[fea_name].value_counts())\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_count\", data_df[fea_name].map(self.ID[fea_name+\"_count\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_count\", data_df[fea_name].map(self.ID[fea_name+\"_count\"]))\n",
    "            data_df[fea_name+\"_count\"] = data_df[fea_name+\"_count\"].fillna(data_df[fea_name+\"_count\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_count\", stage)\n",
    "    \n",
    "    \n",
    "    def getMax(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_max\"][list(set(v[fea_name]))[0]] = v[\"target\"].max()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_max\", data_df[fea_name].map(self.ID[fea_name+\"_max\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_max\", data_df[fea_name].map(self.ID[fea_name+\"_max\"]))\n",
    "            data_df[fea_name+\"_max\"] = data_df[fea_name+\"_max\"].fillna(data_df[fea_name+\"_max\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_max\", stage)\n",
    "    \n",
    "    \n",
    "    def getMin(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_min\"][list(set(v[fea_name]))[0]] = v[\"target\"].min()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_min\", data_df[fea_name].map(self.ID[fea_name+\"_min\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_min\", data_df[fea_name].map(self.ID[fea_name+\"_min\"]))\n",
    "            data_df[fea_name+\"_min\"] = data_df[fea_name+\"_min\"].fillna(data_df[fea_name+\"_min\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_min\", stage)\n",
    "    \n",
    "    \n",
    "    def getMedian(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_median\"][list(set(v[fea_name]))[0]] = v[\"target\"].median()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_median\", data_df[fea_name].map(self.ID[fea_name+\"_median\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_median\", data_df[fea_name].map(self.ID[fea_name+\"_median\"]))\n",
    "            data_df[fea_name+\"_median\"] = data_df[fea_name+\"_median\"].fillna(data_df[fea_name+\"_median\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_median\", stage)\n",
    "    \n",
    "    \n",
    "    def getSum(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_sum\"][list(set(v[fea_name]))[0]] = v[\"target\"].sum()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_sum\", data_df[fea_name].map(self.ID[fea_name+\"_sum\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_sum\", data_df[fea_name].map(self.ID[fea_name+\"_sum\"]))\n",
    "            data_df[fea_name+\"_sum\"] = data_df[fea_name+\"_sum\"].fillna(data_df[fea_name+\"_sum\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_sum\", stage)\n",
    "    \n",
    "    \n",
    "    def getStd(self, data_df, fea_name, stage):\n",
    "        if stage == \"train\":\n",
    "            cs = data_df.loc[:, [fea_name, \"target\"]]\n",
    "            cs = cs.groupby(fea_name)\n",
    "            for k, v in cs:\n",
    "                self.ID[fea_name+\"_std\"][list(set(v[fea_name]))[0]] = v[\"target\"].std()\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_std\", data_df[fea_name].map(self.ID[fea_name+\"_std\"]))\n",
    "        else:\n",
    "            data_df.insert(len(data_df.columns)-1, fea_name+\"_std\", data_df[fea_name].map(self.ID[fea_name+\"_std\"]))\n",
    "            data_df[fea_name+\"_std\"] = data_df[fea_name+\"_std\"].fillna(data_df[fea_name+\"_std\"].value_counts().index[0])\n",
    "        self.add2list(len(index2name)-1, fea_name+\"_std\", stage)\n",
    "        \n",
    "    def newFeature(self, data_df, stage=\"train\"):\n",
    "        # 构造特征\n",
    "        pd.set_option('mode.use_inf_as_na', True)\n",
    "\n",
    "        data_df.insert(len(data_df.columns)-1, \"降价比率\", (data_df[\"新车价\"] - data_df[\"target\"]) / data_df[\"新车价\"])\n",
    "        self.add2list(len(index2name)-1, \"降价比率\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1,\"展销是否为假期\",data_df[\"展销时间\"].map(lambda x: is_holiday(x)))\n",
    "        self.add2list_cat(len(index2name)-1, \"展销是否为假期\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1,\"是否转户\", data_df[\"过户次数\"].map(lambda x: int(x > 0)))\n",
    "        # self.add2list(len(index2name)-1, \"是否转户\", stage)\n",
    "        # self.add2list_cat(len(index2name)-1, \"是否转户\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1, \"注册年份\", data_df[\"注册日期\"].map(lambda x: x.year))\n",
    "        self.add2list(len(index2name)-1, \"注册年份\", stage)\n",
    "\n",
    "        data_df.insert(len(data_df.columns)-1, \"Feature_12_num_sqrt\", data_df[\"Feature_12\"].map(lambda x: eval(x) ** (1./3.)))\n",
    "        self.add2list(len(index2name)-1, \"Feature_12_num_sqrt\", stage)\n",
    "\n",
    "        data_df.insert(len(data_df.columns)-1, \"Feature_12_0\", data_df[\"Feature_12\"].map(lambda x: int(x.split('*')[0])))\n",
    "        self.add2list(len(index2name)-1, \"Feature_12_0\", stage)\n",
    "\n",
    "        data_df.insert(len(data_df.columns)-1, \"Feature_12_1\", data_df[\"Feature_12\"].map(lambda x: int(x.split('*')[1])))\n",
    "        self.add2list(len(index2name)-1, \"Feature_12_1\", stage)\n",
    "\n",
    "        data_df.insert(len(data_df.columns)-1, \"Feature_12_2\", data_df[\"Feature_12\"].map(lambda x: int(x.split('*')[2])))\n",
    "        self.add2list(len(index2name)-1, \"Feature_12_2\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1, \"汽车年龄\", (data_df[\"展销时间\"] - data_df[\"上牌日期\"]).map(lambda x: x.days))\n",
    "        # self.add2list(len(index2name)-1, \"汽车年龄\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1, \"降价次数\", data_df[\"updatePriceTimeJson\"].map(lambda x: len(x)-1))\n",
    "        self.add2list(len(index2name)-1, \"降价次数\", stage)\n",
    "        \n",
    "        data_df.insert(len(data_df.columns)-1, \"降价比例\", data_df[[\"pushPrice\", \"updatePriceTimeJson\"]].apply(lambda x: (x[\"pushPrice\"] - float(list(x[\"updatePriceTimeJson\"].values())[-1])) / x[\"pushPrice\"], axis=1))\n",
    "        self.add2list(len(index2name)-1, \"降价比例\", stage)\n",
    "\n",
    "\n",
    "        self.getSum(data_df, \"车辆所在城市 id\", stage)\n",
    "        \n",
    "        self.getMean(data_df, \"品牌 id\", stage)\n",
    "        self.getCount(data_df, \"品牌 id\", stage)\n",
    "        self.getMax(data_df, \"品牌 id\", stage)\n",
    "        self.getMin(data_df, \"品牌 id\", stage)\n",
    "        self.getMedian(data_df, \"品牌 id\", stage)\n",
    "        self.getSum(data_df, \"品牌 id\", stage)\n",
    "        \n",
    "        # self.getMean(data_df, \"是否转户\", stage)\n",
    "        self.getMean(data_df, \"厂商类型\", stage)\n",
    "        # self.getMean(data_df, \"燃油类型\", stage)\n",
    "        self.getMean(data_df, \"国标码\", stage)\n",
    "        self.getMean(data_df, \"年款\", stage)\n",
    "        self.getMean(data_df, \"变速箱\", stage)\n",
    "        self.getMean(data_df, \"Feature_2\", stage)\n",
    "        # self.getMean(data_df, \"Feature_6\", stage)\n",
    "        # self.getMean(data_df, \"Feature_8\", stage)\n",
    "        # self.getMean(data_df, \"Feature_9\", stage)\n",
    "        # self.getMean(data_df, \"展销是否为假期\", stage)\n",
    "        # self.getCount(data_df, \"车型 id\", stage)\n",
    "        \n",
    "        data_df[\"新车价\"] = np.log(data_df[\"新车价\"])\n",
    "        data_df.insert(len(data_df.columns)-1, \"评估方法1\", np.log(data_df.apply(lambda x: x[\"新车价\"] * (0.15 + 0.85*(1 - 0.11*min(3, max(0., x[\"汽车年龄\"] / 365)) - 0.1 * min(4., max(0, x[\"汽车年龄\"] / 365-3)) - 0.09 * min(4., max(0, x[\"汽车年龄\"] / 365-7)))), axis=1)))\n",
    "        self.add2list(len(index2name)-1, \"评估方法1\", stage)\n",
    "        data_df.insert(len(data_df.columns)-1, \"评估方法2\", np.log(data_df.apply(lambda x: x[\"新车价\"] * (15 - 5 * min(6, max(0, x[\"里程\"])) / 6 - 4 * min(6, max(0, x[\"里程\"]-6)) / 6 - 3 * min(6, max(0, x[\"里程\"]-12)) / 6 - 2 * min(6, max(0, x[\"里程\"]-18)) / 6 - min(6, max(0, x[\"里程\"]-24)) / 6) / 15, axis=1)))\n",
    "        self.add2list(len(index2name)-1, \"评估方法2\", stage)\n",
    "        data_df[\"评估方法2\"].fillna(data_df[\"评估方法1\"], inplace=True)\n",
    "\n",
    "# 生成新特征\n",
    "FE = FeatureEngine()\n",
    "FE.newFeature(tr, \"train\")\n",
    "FE.newFeature(te, \"test\")\n",
    "# FE.crossFeature(tr, \"train\")\n",
    "# FE.crossFeature(te, \"test\")\n",
    "\n",
    "# 取出numerical特征和categories特征\n",
    "tr_x = tr.iloc[:, :-1]\n",
    "tr_y = tr[[\"sday\"]]\n",
    "# te_x = te.iloc[:, :-1]\n",
    "# te_y = te[[\"sday\"]]\n",
    "tr_x_num = tr[numerical].astype(\"float\")\n",
    "te_x_num = te[numerical].astype(\"float\")\n",
    "tr_x_cat = tr[category]\n",
    "te_x_cat = te[category]\n",
    "\n",
    "# 特征归一化： (x-min) / (max-min)\n",
    "for _ in tr_x_num:\n",
    "    if not _ in [\"厂商类型\", \"载客人数\", \"Feature_8\", \"是否转户\"]:\n",
    "        min_ = tr_x_num[_].min()\n",
    "        max_ = tr_x_num[_].max()\n",
    "        tr_x_num[_] = (tr_x_num[_] - min_) / (max_ - min_)\n",
    "        te_x_num[_] = (te_x_num[_] - min_) / (max_ - min_)\n",
    "\n",
    "tr_x_num_array = np.array(tr_x_num)\n",
    "tr_y_array = np.array(tr_y)\n",
    "te_x_num_array = np.array(te_x_num)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "tr_num = pd.concat([tr_x_num, tr_y], axis=1)\n",
    "sns.heatmap(tr_num.corr(), annot=True, fmt='.2f')\n",
    "plt.savefig(root2 / \"Q3_2.jpg\")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from pprint import pprint\n",
    "\n",
    "# 优化结果：模型参数(使用hebo进行参数优化，因为一次优化的时间消耗大，因此将优化结果存了下来)\n",
    "lgbParams = {'learning_rate': 0.010035982594961306, 'subsample': 0.35028462644730735, 'colsample_bytree': 0.3796965007412236, 'reg_lambda': 0.0006865914755758266, 'reg_alpha': 0.0026163019185854106, 'num_leaves': 189, 'max_depth': 37, 'n_estimators': 1965, 'objective': 'mse', 'min_child_samples': 25}\n",
    "xgbParams = {'learning_rate': 0.010211967985703159, 'subsample': 0.30001115463199896, 'colsample_bytree': 0.48460986059028555, 'lambda': 0.032780563044634864, 'alpha': 0.06981554843421037, 'max_depth': 26, 'min_child_weight': 10, 'n_estimators': 1845}\n",
    "catParams = {'learning_rate': 0.08491056514750207, 'l2_leaf_reg': 1, 'max_depth': 8, 'n_estimators': 2000, 'min_child_samples': 25, 'loss_function': 'RMSE', 'verbose': False, 'task_type': 'CPU'}\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Fit model using each importance as a threshold\n",
    "model_XGB = XGBRegressor(**xgbParams).fit(tr_x_num, tr_y)\n",
    "thresholds = sort(model_XGB.feature_importances_)\n",
    "f_i = dict(zip(numerical, model_XGB.feature_importances_))\n",
    "f_i = sorted(f_i.items(), key=lambda x: x[1])\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "plot_importance(model_XGB, importance_type=\"weight\", ax=ax)\n",
    "plt.savefig(root2 / \"Q3_3.jpg\")\n",
    "\n",
    "for name, thresh in f_i:\n",
    "  # select features using threshold\n",
    "    selection = SelectFromModel(model_XGB, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(tr_x_num)\n",
    "  # train model\n",
    "    selection_model = XGBRegressor(**xgbParams)\n",
    "    selection_model.fit(select_X_train, tr_y)\n",
    "  # eval model\n",
    "    select_X_test = selection.transform(te_x_num)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    accuracy = metric(pt_tr.inverse_transform(y_pred.reshape(-1, 1)) + te[[\"fday\"]], np.array(te[[\"成交周期\"]]))\n",
    "    print(\"Feature Name=%s Thresh=%.3f, n=%d, Accuracy: %.2f\" % (name, thresh, select_X_train.shape[1], accuracy))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "import numpy as np\n",
    " \n",
    "# stacking\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    " \n",
    "    # 将原来的模型clone出来，并且进行实现fit功能\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    " \n",
    "        #对于每个模型，使用交叉验证的方法来训练初级学习器，并且得到次级训练集\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                self.base_models_[i].append(instance)\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    " \n",
    "        # 使用次级训练集来训练次级学习器\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    " \n",
    "    #在上面的fit方法当中，我们已经将我们训练出来的初级学习器和次级学习器保存下来了\n",
    "    #predict的时候只需要用这些学习器构造我们的次级预测数据集并且进行预测就可以了\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "base_models = [\n",
    "               CatBoostRegressor(**catParams),\n",
    "               LGBMRegressor(**lgbParams),\n",
    "               XGBRegressor(**xgbParams),\n",
    "              ]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "stacking_model = StackingAveragedModels(base_models=base_models, meta_model=meta_model)\n",
    "\n",
    "stacking_model.fit(tr_x_num_array, tr_y_array.ravel())\n",
    "\n",
    "y_predict = stacking_model.predict(te_x_num_array)\n",
    "\n",
    "res = pt_tr.inverse_transform(y_predict.reshape(-1, 1)) + te[[\"fday\"]]\n",
    "\n",
    "res.insert(0, \"carid\", te[\"carid\"])\n",
    "\n",
    "res[\"fday\"] = res[\"fday\"].astype(\"int\")\n",
    "\n",
    "res.to_csv(root / \"Q3.txt\", index=False, sep='\\t', header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
